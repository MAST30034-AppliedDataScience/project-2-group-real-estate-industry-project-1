{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from pyspark.sql import SparkSession\n",
    "from shapely.geometry import Point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ../../data/landing/PTV/ already exists, skipping creation.\n",
      "Directory ../../data/raw/PTV/ already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_relative_dir = '../../data/landing/PTV/'\n",
    "output_absolute_dir = '../../data/raw/PTV/'\n",
    "\n",
    "\n",
    "if not os.path.exists(output_relative_dir):\n",
    "    os.makedirs(output_relative_dir)\n",
    "    print(f\"Directory {output_relative_dir} created.\")\n",
    "else:\n",
    "    print(f\"Directory {output_relative_dir} already exists, skipping creation.\")\n",
    "\n",
    "if not os.path.exists(output_absolute_dir):\n",
    "    os.makedirs(output_absolute_dir)\n",
    "    print(f\"Directory {output_absolute_dir} created.\")\n",
    "else:\n",
    "    print(f\"Directory {output_absolute_dir} already exists, skipping creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file...\n",
      "Extracting file...\n",
      "File downloaded and extracted successfully.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://data.ptv.vic.gov.au/downloads/gtfs.zip\"\n",
    "download_path = \"../../data/landing/PTV/gtfs.zip\"\n",
    "extract_to_path = \"../../data/landing/PTV/\"\n",
    "\n",
    "os.makedirs(extract_to_path, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(download_path):\n",
    "    print(\"Downloading file...\")\n",
    "    response = requests.get(url)\n",
    "    with open(download_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    print(\"Extracting file...\")\n",
    "    with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to_path)\n",
    "\n",
    "    print(\"File downloaded and extracted successfully.\")\n",
    "else:\n",
    "    print(\"Zip file already exists, skipping download and extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 12:27:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted stops.txt from ../../data/landing/PTV/1/google_transit.zip\n",
      "Converted ../../data/landing/PTV/1/stops.txt to ../../data/raw/PTV/un_preprocess/Regional Train.parquet\n",
      "Extracted stops.txt from ../../data/landing/PTV/2/google_transit.zip\n",
      "Converted ../../data/landing/PTV/2/stops.txt to ../../data/raw/PTV/un_preprocess/Metropolitan Train.parquet\n",
      "Extracted stops.txt from ../../data/landing/PTV/3/google_transit.zip\n",
      "Converted ../../data/landing/PTV/3/stops.txt to ../../data/raw/PTV/un_preprocess/Metropolitan Tram.parquet\n",
      "Extracted stops.txt from ../../data/landing/PTV/4/google_transit.zip\n",
      "Converted ../../data/landing/PTV/4/stops.txt to ../../data/raw/PTV/un_preprocess/Metropolitan Bus.parquet\n",
      "Extracted stops.txt from ../../data/landing/PTV/5/google_transit.zip\n",
      "Converted ../../data/landing/PTV/5/stops.txt to ../../data/raw/PTV/un_preprocess/Regional Coach.parquet\n",
      "Extracted stops.txt from ../../data/landing/PTV/6/google_transit.zip\n",
      "Converted ../../data/landing/PTV/6/stops.txt to ../../data/raw/PTV/un_preprocess/Regional Bus.parquet\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Convert stops.txt to Parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 定义基础目录路径\n",
    "ptv_dir = '../../data/landing/PTV/'\n",
    "output_dir = '../../data/raw/PTV/un_preprocess/'\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 文件夹与对应输出文件名的映射\n",
    "folders_to_process = {\n",
    "    \"1\": \"Regional Train\",\n",
    "    \"2\": \"Metropolitan Train\",\n",
    "    \"3\": \"Metropolitan Tram\",\n",
    "    \"4\": \"Metropolitan Bus\",\n",
    "    \"5\": \"Regional Coach\",\n",
    "    \"6\": \"Regional Bus\"\n",
    "}\n",
    "\n",
    "# 循环处理每个文件夹\n",
    "for folder, parquet_name in folders_to_process.items():\n",
    "    folder_path = os.path.join(ptv_dir, folder)\n",
    "    \n",
    "    # 查找压缩包文件\n",
    "    zip_files = [f for f in os.listdir(folder_path) if f.endswith('.zip')]\n",
    "    \n",
    "    if zip_files:\n",
    "        zip_file_path = os.path.join(folder_path, zip_files[0])\n",
    "        \n",
    "        # 解压 stops.txt 文件\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            if 'stops.txt' in zip_ref.namelist():\n",
    "                zip_ref.extract('stops.txt', folder_path)\n",
    "                print(f\"Extracted stops.txt from {zip_file_path}\")\n",
    "            else:\n",
    "                print(f\"stops.txt not found in {zip_file_path}\")\n",
    "        \n",
    "        # 读取解压后的 stops.txt 文件\n",
    "        stops_txt_path = os.path.join(folder_path, 'stops.txt')\n",
    "        if os.path.exists(stops_txt_path):\n",
    "            # 使用 PySpark 读取 stops.txt 文件\n",
    "            df = spark.read.csv(stops_txt_path, header=True, inferSchema=True)\n",
    "            \n",
    "            # 定义 parquet 文件的路径\n",
    "            parquet_file_path = os.path.join(output_dir, f'{parquet_name}.parquet')\n",
    "            \n",
    "            # 转换为 parquet 并保存\n",
    "            df.write.parquet(parquet_file_path, mode='overwrite')\n",
    "            print(f\"Converted {stops_txt_path} to {parquet_file_path}\")\n",
    "        else:\n",
    "            print(f\"stops.txt not found in {folder_path}\")\n",
    "    else:\n",
    "        print(f\"No zip file found in {folder_path}\")\n",
    "\n",
    "# 关闭 SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 12:27:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/jadexu/Documents/GitHub/project-2-group-real-estate-industry-project-1/data/raw/PTV/Un_preprocess_PTV/1 - Regional Train.parquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 2. 读取 parquet 文件\u001b[39;00m\n\u001b[1;32m      5\u001b[0m parquet_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/raw/PTV/Un_preprocess_PTV/1 - Regional Train.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m stops_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 3. 选择经纬度列并转换为 Pandas DataFrame\u001b[39;00m\n\u001b[1;32m      9\u001b[0m stops_pd \u001b[38;5;241m=\u001b[39m stops_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_lat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_lon\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv2/lib/python3.10/site-packages/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv2/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv2/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/Users/jadexu/Documents/GitHub/project-2-group-real-estate-industry-project-1/data/raw/PTV/Un_preprocess_PTV/1 - Regional Train.parquet."
     ]
    }
   ],
   "source": [
    "# 1. 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"Geometric Point Map with Folium\").getOrCreate()\n",
    "\n",
    "# 2. 读取 parquet 文件\n",
    "parquet_path = '../../data/raw/PTV/Un_preprocess_PTV/1 - Regional Train.parquet'\n",
    "stops_df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# 3. 选择经纬度列并转换为 Pandas DataFrame\n",
    "stops_pd = stops_df.select(\"stop_lat\", \"stop_lon\").toPandas()\n",
    "\n",
    "# 4. 将 Pandas DataFrame 转换为 GeoDataFrame\n",
    "geometry = [Point(xy) for xy in zip(stops_pd['stop_lon'], stops_pd['stop_lat'])]\n",
    "gdf = gpd.GeoDataFrame(stops_pd, geometry=geometry)\n",
    "\n",
    "# 5. 定义坐标参考系统 (CRS)\n",
    "gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# 6. 创建一个 Folium 地图对象，中心定位在站点的平均位置\n",
    "m = folium.Map(location=[gdf['stop_lat'].mean(), gdf['stop_lon'].mean()], zoom_start=12)\n",
    "\n",
    "# 7. 添加每个站点作为固定大小的 CircleMarker\n",
    "for idx, row in gdf.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['stop_lat'], row['stop_lon']],\n",
    "        radius=3,  # 点的半径，单位为像素\n",
    "        color='blue',  # 边框颜色\n",
    "        fill=True,\n",
    "        fill_color='blue',  # 填充颜色\n",
    "        fill_opacity=0.7\n",
    "    ).add_to(m)\n",
    "\n",
    "# 8. 保存地图\n",
    "# map_path = ''\n",
    "# m.save(map_path)\n",
    "\n",
    "# 在浏览器中打开生成的 HTML 文件\n",
    "# print(f\"Map has been generated and saved to {map_path}. Open it in a browser to view.\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 12:27:22 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have consistent features:\n",
      "['stop_lat', 'stop_name', 'stop_id', 'stop_lon']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "def check_parquet_features(directory):\n",
    "    spark = SparkSession.builder.appName(\"Check Parquet Features\").getOrCreate()\n",
    "    \n",
    "    # 初始化集合来存储所有文件的列\n",
    "    columns_set = set()\n",
    "    parquet_files = []\n",
    "\n",
    "    # 遍历目录，找到所有 .parquet 文件\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".parquet\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                parquet_files.append(file_path)\n",
    "\n",
    "    # 遍历所有 .parquet 文件，获取列名\n",
    "    for file_path in parquet_files:\n",
    "        df = spark.read.parquet(file_path)\n",
    "        columns = set(df.columns)\n",
    "        \n",
    "        # 将首个文件的列集合设为基准\n",
    "        if not columns_set:\n",
    "            columns_set = columns\n",
    "        else:\n",
    "            # 如果当前文件的列集合与基准不一致\n",
    "            if columns != columns_set:\n",
    "                spark.stop()\n",
    "                return False, (columns_set, columns)\n",
    "\n",
    "    spark.stop()\n",
    "    return True, list(columns_set)\n",
    "\n",
    "# 指定包含 .parquet 文件的目录（相对路径）\n",
    "directory = '../../data/raw/PTV/Un_preprocess_PTV/'\n",
    "\n",
    "# 测试文件特征是否一致\n",
    "features_consistent, columns_info = check_parquet_features(directory)\n",
    "\n",
    "if features_consistent:\n",
    "    print(\"All files have consistent features:\")\n",
    "    print(columns_info)\n",
    "else:\n",
    "    print(\"Files have different features:\")\n",
    "    print(\"Base columns:\", columns_info[0])\n",
    "    print(\"Different columns:\", columns_info[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
