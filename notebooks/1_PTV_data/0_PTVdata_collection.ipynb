{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from pyspark.sql import SparkSession\n",
    "from shapely.geometry import Point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ../../data/landing/PTV/ already exists, skipping creation.\n",
      "Directory ../../data/raw/PTV/ already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_relative_dir = '../../data/landing/PTV/'\n",
    "output_absolute_dir = '../../data/raw/PTV/'\n",
    "\n",
    "\n",
    "if not os.path.exists(output_relative_dir):\n",
    "    os.makedirs(output_relative_dir)\n",
    "    print(f\"Directory {output_relative_dir} created.\")\n",
    "else:\n",
    "    print(f\"Directory {output_relative_dir} already exists, skipping creation.\")\n",
    "\n",
    "if not os.path.exists(output_absolute_dir):\n",
    "    os.makedirs(output_absolute_dir)\n",
    "    print(f\"Directory {output_absolute_dir} created.\")\n",
    "else:\n",
    "    print(f\"Directory {output_absolute_dir} already exists, skipping creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip file already exists, skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://data.ptv.vic.gov.au/downloads/gtfs.zip\"\n",
    "download_path = \"../../data/landing/PTV/gtfs.zip\"\n",
    "extract_to_path = \"../../data/landing/PTV/\"\n",
    "\n",
    "os.makedirs(extract_to_path, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(download_path):\n",
    "    print(\"Downloading file...\")\n",
    "    response = requests.get(url)\n",
    "    with open(download_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    print(\"Extracting file...\")\n",
    "    with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to_path)\n",
    "\n",
    "    print(\"File downloaded and extracted successfully.\")\n",
    "else:\n",
    "    print(\"Zip file already exists, skipping download and extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/12 18:47:34 WARN Utils: Your hostname, yoga resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/09/12 18:47:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/12 18:47:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted stops.txt from ../../data/landing/PTV/1/google_transit.zip\n",
      "Converted ../../data/landing/PTV/1/stops.txt to ../../data/raw/PTV/un_preprocess/Regional Train.parquet\n",
      "Extracted stops.txt from ../../data/landing/PTV/2/google_transit.zip\n",
      "Converted ../../data/landing/PTV/2/stops.txt to ../../data/raw/PTV/un_preprocess/Metropolitan Train.parquet\n",
      "Extracted stops.txt from ../../data/landing/PTV/3/google_transit.zip\n",
      "Converted ../../data/landing/PTV/3/stops.txt to ../../data/raw/PTV/un_preprocess/Metropolitan Tram.parquet\n",
      "Extracted stops.txt from ../../data/landing/PTV/4/google_transit.zip\n",
      "Converted ../../data/landing/PTV/4/stops.txt to ../../data/raw/PTV/un_preprocess/Metropolitan Bus.parquet\n",
      "Extracted stops.txt from ../../data/landing/PTV/5/google_transit.zip\n",
      "Converted ../../data/landing/PTV/5/stops.txt to ../../data/raw/PTV/un_preprocess/Regional Coach.parquet\n",
      "Extracted stops.txt from ../../data/landing/PTV/6/google_transit.zip\n",
      "Converted ../../data/landing/PTV/6/stops.txt to ../../data/raw/PTV/un_preprocess/Regional Bus.parquet\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Define base directory paths\n",
    "ptv_dir = '../../data/landing/PTV/'\n",
    "output_dir = '../../data/raw/PTV/un_preprocess/'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Mapping of folders to corresponding output parquet filenames\n",
    "folders_to_process = {\n",
    "    \"1\": \"Regional Train\",\n",
    "    \"2\": \"Metropolitan Train\",\n",
    "    \"3\": \"Metropolitan Tram\",\n",
    "    \"4\": \"Metropolitan Bus\",\n",
    "    \"5\": \"Regional Coach\",\n",
    "    \"6\": \"Regional Bus\"\n",
    "}\n",
    "\n",
    "# Loop through each folder to process\n",
    "for folder, parquet_name in folders_to_process.items():\n",
    "    folder_path = os.path.join(ptv_dir, folder)\n",
    "    \n",
    "    # Find zip files in the folder\n",
    "    zip_files = [f for f in os.listdir(folder_path) if f.endswith('.zip')]\n",
    "    \n",
    "    if zip_files:\n",
    "        zip_file_path = os.path.join(folder_path, zip_files[0])\n",
    "        \n",
    "        # Extract stops.txt from the zip file\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            if 'stops.txt' in zip_ref.namelist():\n",
    "                zip_ref.extract('stops.txt', folder_path)\n",
    "                print(f\"Extracted stops.txt from {zip_file_path}\")\n",
    "            else:\n",
    "                print(f\"stops.txt not found in {zip_file_path}\")\n",
    "        \n",
    "        # Read the extracted stops.txt file\n",
    "        stops_txt_path = os.path.join(folder_path, 'stops.txt')\n",
    "        if os.path.exists(stops_txt_path):\n",
    "            # Use PySpark to read the stops.txt file\n",
    "            df = spark.read.csv(stops_txt_path, header=True, inferSchema=True)\n",
    "            \n",
    "            # Define the path for the output parquet file\n",
    "            parquet_file_path = os.path.join(output_dir, f'{parquet_name}.parquet')\n",
    "            \n",
    "            # Convert the DataFrame to parquet and save it\n",
    "            df.write.parquet(parquet_file_path, mode='overwrite')\n",
    "            print(f\"Converted {stops_txt_path} to {parquet_file_path}\")\n",
    "        else:\n",
    "            print(f\"stops.txt not found in {folder_path}\")\n",
    "    else:\n",
    "        print(f\"No zip file found in {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have consistent features:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def check_parquet_features(directory):\n",
    "    columns_set = set()\n",
    "    parquet_files = []\n",
    "\n",
    "    # Traverse the directory to find all .parquet files\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".parquet\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                parquet_files.append(file_path)\n",
    "\n",
    "    # Iterate over all .parquet files to get column names\n",
    "    for file_path in parquet_files:\n",
    "        df = spark.read.parquet(file_path)\n",
    "        columns = set(df.columns)\n",
    "        \n",
    "        # Set the columns of the first file as the baseline\n",
    "        if not columns_set:\n",
    "            columns_set = columns\n",
    "        else:\n",
    "            # check if the columns of the current file are inconsistent with the baseline\n",
    "            if columns != columns_set:\n",
    "                spark.stop()\n",
    "                return False, (columns_set, columns)\n",
    "\n",
    "    spark.stop()\n",
    "    return True, list(columns_set)\n",
    "\n",
    "# Specify the directory containing .parquet files (relative path)\n",
    "directory = '../../data/raw/PTV/Un_preprocess/'\n",
    "\n",
    "# Test if the features of the files are consistent\n",
    "features_consistent, columns_info = check_parquet_features(directory)\n",
    "\n",
    "if features_consistent:\n",
    "    print(\"All files have consistent features:\")\n",
    "    print(columns_info)\n",
    "else:\n",
    "    print(\"Files have different features:\")\n",
    "    print(\"Base columns:\", columns_info[0])\n",
    "    print(\"Different columns:\", columns_info[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
