{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "scripts_path = os.path.join(current_dir, '../../scripts')\n",
    "sys.path.append(os.path.abspath(scripts_path))\n",
    "import PTV_preprocess_function as process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/04 16:40:36 WARN Utils: Your hostname, yoga resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/09/04 16:40:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/04 16:40:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to save data after simple preprocessing\n",
    "preprocess = 'preprocessed'\n",
    "unprepro = 'un_preprocess'\n",
    "raw_dir = '../../data/raw/PTV/'\n",
    "\n",
    "if not os.path.exists(os.path.join(raw_dir, preprocess)):\n",
    "    os.makedirs(os.path.join(raw_dir, preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "input_dir = f\"{raw_dir}/{unprepro}\"\n",
    "output_dir = f\"{raw_dir}/{preprocess}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Union dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metropolitan Train.parquet - Shape: (221 rows, 4 columns)\n",
      "Regional Bus.parquet - Shape: (6854 rows, 4 columns)\n",
      "Metropolitan Tram.parquet - Shape: (1626 rows, 4 columns)\n",
      "Regional Coach.parquet - Shape: (893 rows, 4 columns)\n",
      "Metropolitan Bus.parquet - Shape: (18612 rows, 4 columns)\n",
      "Regional Train.parquet - Shape: (110 rows, 4 columns)\n"
     ]
    }
   ],
   "source": [
    "# initialize an empty dataframe to store merged data\n",
    "all_data = None\n",
    "trans_type = []\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "\n",
    "    # read file\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    df = spark.read.parquet(file_path)\n",
    "    process.print_dataset_shape(file, df)    \n",
    "    # create new feature - transportation_type\n",
    "    transportation_type = file.replace('.parquet', '')\n",
    "    trans_type.append(transportation_type)\n",
    "    \n",
    "    df = df.withColumn('public_transportation_type', lit(transportation_type))\n",
    "\n",
    "    # union dataset\n",
    "    if all_data is None:\n",
    "        all_data = df\n",
    "    else:\n",
    "        all_data = all_data.union(df)\n",
    "\n",
    "# save as parquet\n",
    "all_data.write.mode('overwrite').parquet(f\"{output_dir}/PTV_union.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "parquet_path = f\"{output_dir}/PTV_union.parquet\"\n",
    "sdf = spark.read.parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stop_id: integer (nullable = true)\n",
      " |-- stop_name: string (nullable = true)\n",
      " |-- stop_lat: double (nullable = true)\n",
      " |-- stop_lon: double (nullable = true)\n",
      " |-- public_transportation_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset shape:  - Shape: (28316 rows, 5 columns)\n"
     ]
    }
   ],
   "source": [
    "process.print_dataset_shape(\"original dataset shape: \", sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/04 16:40:42 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+-------------------+------------------+--------------------------+\n",
      "|summary|           stop_id|           stop_name|           stop_lat|          stop_lon|public_transportation_type|\n",
      "+-------+------------------+--------------------+-------------------+------------------+--------------------------+\n",
      "|  count|             28316|               28316|              28316|             28316|                     28316|\n",
      "|   mean|22286.645359514056|                NULL|-37.691457563928594|144.92578657647985|                      NULL|\n",
      "| stddev|15951.415071739373|                NULL| 0.6274758685737729|0.8152798667253476|                      NULL|\n",
      "|    min|                 4|0-Bourke St/Sprin...|   -38.777123382502|  138.595751207359|          Metropolitan Bus|\n",
      "|    max|             52183|opp 68 The Elms B...|  -34.1652286164826|   150.17814893804|            Regional Train|\n",
      "+-------+------------------+--------------------+-------------------+------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing value\n",
    "process.check_missing_values(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no duplicate\n"
     ]
    }
   ],
   "source": [
    "# check duplicate record\n",
    "duplicate_rows = df.groupBy(df.columns).count().filter(\"count > 1\")\n",
    "\n",
    "if duplicate_rows.count() > 0:\n",
    "    duplicate_rows.show()\n",
    "else:\n",
    "    print(\"no duplicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----+\n",
      "|public_transportation_type|count|\n",
      "+--------------------------+-----+\n",
      "|          Metropolitan Bus|18612|\n",
      "|              Regional Bus| 6854|\n",
      "|         Metropolitan Tram| 1626|\n",
      "|            Regional Coach|  893|\n",
      "|        Metropolitan Train|  221|\n",
      "|            Regional Train|  110|\n",
      "+--------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.groupBy(\"public_transportation_type\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
